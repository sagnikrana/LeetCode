{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load general utilities\n",
    "# ----------------------\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.axes as ax\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import seaborn as sns\n",
    "pd.set_option('display.max_rows',99999999)\n",
    "pd.set_option('display.max_columns',100)\n",
    "pd.set_option('display.max_colwidth',100)\n",
    "pd.set_option('display.width',None)\n",
    "\n",
    "# Load sklearn utilities\n",
    "# ----------------------\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, roc_curve, brier_score_loss, mean_squared_error, r2_score\n",
    "\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# Load classifiers\n",
    "# ----------------\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Other Packages\n",
    "# --------------\n",
    "from scipy.stats import kendalltau\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "from scipy.interpolate import spline\n",
    "\n",
    "# Load debugger, if required\n",
    "#import pixiedust\n",
    "pd.options.mode.chained_assignment = None #'warn'\n",
    "\n",
    "# suppress all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_classification(model, data_dict,\n",
    "                          cv_parameters = {},\n",
    "                          model_name = None,\n",
    "                          random_state = default_seed,\n",
    "                          output_to_file = True,\n",
    "                          print_to_screen = True):\n",
    "    '''\n",
    "    This function will fit a classification model to data and print various evaluation\n",
    "    measures. It expects the following parameters\n",
    "      - model: an sklearn model object\n",
    "      - data_dict: the dictionary containing both training and testing data;\n",
    "                   returned by the prepare_data function\n",
    "      - cv_parameters: a dictionary of parameters that should be optimized\n",
    "                       over using cross-validation. Specifically, each named\n",
    "                       entry in the dictionary should correspond to a parameter,\n",
    "                       and each element should be a list containing the values\n",
    "                       to optimize over\n",
    "      - model_name: the name of the model being fit, for printouts\n",
    "      - random_state: the random seed to use\n",
    "      - output_to_file: if the results will be saved to the output file\n",
    "      - print_to_screen: if the results will be printed on screen\n",
    "    \n",
    "    If the model provided does not have a predict_proba function, we will\n",
    "    simply print accuracy diagnostics and return.\n",
    "    \n",
    "    If the model provided does have a predict_proba function, we first\n",
    "    figure out the optimal threshold that maximizes the accuracy and\n",
    "    print out accuracy diagnostics. We then print an ROC curve, sensitivity/\n",
    "    specificity curve, and calibration curve.\n",
    "    \n",
    "    This function returns a dictionary with the following entries\n",
    "      - model: the best fitted model\n",
    "      - y_pred: predictions for the test set\n",
    "      - y_pred_probs: probability predictions for the test set, if the model\n",
    "                      supports them\n",
    "      - y_pred_score: prediction scores for the test set, if the model does not \n",
    "                      output probabilities.\n",
    "    '''\n",
    "        \n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # --------------------------\n",
    "    #   Step 1 - Load the data\n",
    "    # --------------------------\n",
    "    X_train = data_dict['X_train']\n",
    "    y_train = data_dict['y_train']\n",
    "    \n",
    "    X_test = data_dict['X_test']\n",
    "    y_test = data_dict['y_test']\n",
    "    \n",
    "    filter_train = data_dict['train_set']    \n",
    "  \n",
    "    # --------------------------\n",
    "    #   Step 2 - Fit the model\n",
    "    # --------------------------\n",
    "\n",
    "    cv_model = GridSearchCV(model, cv_parameters)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    cv_model.fit(X_train, y_train)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    best_model = cv_model.best_estimator_\n",
    "    \n",
    "    if print_to_screen:\n",
    "\n",
    "        if model_name != None:\n",
    "            print(\"=========================================================\")\n",
    "            print(\"  Model: \" + model_name)\n",
    "            print(\"=========================================================\")\n",
    "\n",
    "        print(\"Fit time: \" + str(round(end_time - start_time, 2)) + \" seconds\")\n",
    "        print(\"Optimal parameters:\")\n",
    "        print(cv_model.best_params_)\n",
    "        print(\"\")\n",
    "    \n",
    "    # -------------------------------\n",
    "    #   Step 3 - Evaluate the model\n",
    "    # -------------------------------\n",
    "    \n",
    "    # If possible, make probability predictions\n",
    "    try:\n",
    "        y_pred_probs = best_model.predict_proba(X_test)[:,1]\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, y_pred_probs)\n",
    "        \n",
    "        probs_predicted = True\n",
    "    except:\n",
    "        probs_predicted = False\n",
    "    \n",
    "    # Make predictions; if we were able to find probabilities, use\n",
    "    # the threshold that maximizes the accuracy in the training set.\n",
    "    # If not, just use the learner's predict function\n",
    "    if probs_predicted:\n",
    "        y_train_pred_probs = best_model.predict_proba(X_train)[:,1]\n",
    "        fpr_train, tpr_train, thresholds_train = roc_curve(y_train, y_train_pred_probs)\n",
    "        \n",
    "        true_pos_train = tpr_train*(y_train.sum())\n",
    "        true_neg_train = (1 - fpr_train) *(1-y_train).sum()\n",
    "        \n",
    "        best_threshold_index = np.argmax(true_pos_train + true_neg_train)\n",
    "        best_threshold = 1 if best_threshold_index == 0 else thresholds_train[ best_threshold_index ]\n",
    "        \n",
    "        if print_to_screen:\n",
    "            print(\"Accuracy-maximizing threshold was: \" + str(best_threshold))\n",
    "        \n",
    "        y_pred = (y_pred_probs > best_threshold)\n",
    "    else:\n",
    "        y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    if print_to_screen:\n",
    "        print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n",
    "        print(classification_report(y_test, y_pred, target_names =['No default', 'Default'], digits = 4))\n",
    "\n",
    "    if print_to_screen:\n",
    "        if probs_predicted:        \n",
    "            plt.figure(figsize = (13, 4.5))\n",
    "            plt.subplot(2, 2, 1)\n",
    "\n",
    "            plt.title(\"ROC Curve (AUC = %0.2f)\"% roc_auc_score(y_test, y_pred_probs))\n",
    "            plt.plot(fpr, tpr, 'b')\n",
    "            plt.plot([0,1],[0,1],'r--')\n",
    "            plt.xlim([0,1]); plt.ylim([0,1])\n",
    "            plt.ylabel('True Positive Rate')\n",
    "            plt.xlabel('False Positive Rate')\n",
    "\n",
    "            plt.subplot(2, 2, 3)\n",
    "\n",
    "            plt.plot(thresholds, tpr, 'b', label = 'Sensitivity')\n",
    "            plt.plot(thresholds, 1 -fpr, 'r', label = 'Specificity')\n",
    "            plt.legend(loc = 'lower right')\n",
    "            plt.xlim([0,1]); plt.ylim([0,1])\n",
    "            plt.xlabel('Threshold')\n",
    "\n",
    "            plt.subplot(2, 2, 2)\n",
    "\n",
    "            fp_0, mpv_0 = calibration_curve(y_test, y_pred_probs, n_bins = 10)\n",
    "            plt.plot([0,1], [0,1], 'k:', label='Perfectly calibrated')\n",
    "            plt.plot(mpv_0, fp_0, 's-')\n",
    "            plt.ylabel('Fraction of Positives')\n",
    "            plt.xlim([0,1]); plt.ylim([0,1])\n",
    "            plt.legend(loc ='upper left')\n",
    "            \n",
    "            plt.subplot(2, 2, 4)\n",
    "            plt.hist(y_pred_probs, range=(0, 1), bins=10, histtype=\"step\", lw=2)\n",
    "            plt.xlim([0,1]); plt.ylim([0,20000])\n",
    "            plt.xlabel('Mean Predicted Probability')\n",
    "            plt.ylabel('Count')\n",
    "            \n",
    "            #plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "    # Additional Score Check\n",
    "    if probs_predicted:\n",
    "        y_train_score = y_train_pred_probs\n",
    "    else:\n",
    "        y_train_score = best_model.decision_function(X_train)\n",
    "        \n",
    "    tau, p_value = kendalltau(y_train_score, data.grade[filter_train])\n",
    "    if print_to_screen:\n",
    "        print(\"\")\n",
    "        print(\"Similarity to LC grade ranking: \", tau)\n",
    "    \n",
    "    if probs_predicted:\n",
    "        brier_score = brier_score_loss(y_test, y_pred_probs)\n",
    "        if print_to_screen:\n",
    "            print(\"Brier score:\", brier_score)\n",
    "    \n",
    "    # Return the model predictions, and the\n",
    "    # test set\n",
    "    # -------------------------------------\n",
    "    out = {'model':best_model, 'y_pred_labels':y_pred}\n",
    "    \n",
    "    if probs_predicted:\n",
    "        out.update({'y_pred_probs':y_pred_probs})\n",
    "    else:\n",
    "        y_pred_score = best_model.decision_function(X_test)\n",
    "        out.update({'y_pred_score':y_pred_score})\n",
    "        \n",
    "    # Output results to file\n",
    "    # ----------------------\n",
    "    if probs_predicted and output_to_file:\n",
    "        # Check whether any of the CV parameters are on the edge of\n",
    "        # the search space\n",
    "        opt_params_on_edge = find_opt_params_on_edge(cv_model)\n",
    "        dump_to_output(model_name + \"::search_on_edge\", opt_params_on_edge)\n",
    "        if print_to_screen:\n",
    "            print(\"Were parameters on edge? : \" + str(opt_params_on_edge))\n",
    "        \n",
    "        # Find out how different the scores are for the different values\n",
    "        # tested for by cross-validation. If they're not too different, then\n",
    "        # even if the parameters are off the edge of the search grid, we should\n",
    "        # be ok\n",
    "        score_variation = find_score_variation(cv_model)\n",
    "        dump_to_output(model_name + \"::score_variation\", score_variation)\n",
    "        if print_to_screen:\n",
    "            print(\"Score variations around CV search grid : \" + str(score_variation))\n",
    "        \n",
    "        # Print out all the scores\n",
    "        dump_to_output(model_name + \"::all_cv_scores\", str(cv_model.cv_results_['mean_test_score']))\n",
    "        if print_to_screen:\n",
    "            print( str(cv_model.cv_results_['mean_test_score']) )\n",
    "        \n",
    "        # Dump the AUC to file\n",
    "        dump_to_output(model_name + \"::roc_auc\", roc_auc_score(y_test, y_pred_probs) )\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train and test a l_1 regularized logistic regression classifier\n",
    "\n",
    "l2_logistic = LogisticRegression(penalty='l2')\n",
    "cv_parameters = {'C':[10, 1, .1, .001]}\n",
    "\n",
    "l2_logistic = fit_classification(l2_logistic,data_dict,cv_parameters,'Logistic Regressoin',\n",
    "                                 default_seed, True, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train and test a naive bayes classifier\n",
    "gnb = GaussianNB()\n",
    "gnb = fit_classification(gnb,data_dict,{},'Gaussian Model',default_seed,True,True)\n",
    "gnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train and test a decision tree classifier\n",
    "\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "cv_parameters = {'max_depth':[3]}\n",
    "\n",
    "decision_tree = fit_classification(decision_tree,data_dict,cv_parameters,\n",
    "                                   'Decision Tree',default_seed,True,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train and test a random forest classifier\n",
    "\n",
    "random_forest = RandomForestClassifier()\n",
    "cv_parameters = {'n_estimators':[50, 100, 150, 200, 250, 300],\n",
    "                'max_depth':[3, 4, 5, 7, 8, None]}\n",
    "\n",
    "random_forest = fit_classification(random_forest,data_dict,cv_parameters,\n",
    "                                   'Random Forest',default_seed,True,True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
